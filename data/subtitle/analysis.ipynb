{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 형태소 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 파일 읽기\n",
    "def read_txt_file(file_name):\n",
    "  print(\"[파일명] \" + file_name)\n",
    "  text = \"\"\n",
    "  \n",
    "  # 파일 열기\n",
    "  with open(file_path + file_name, \"r\", encoding=\"UTF8\") as txt_file:\n",
    "    # 파일 라인별로 텍스트만 추출\n",
    "    for line in txt_file:\n",
    "      line = line.strip()\n",
    "      text += line\n",
    "  return text\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\sodud\\\\study\\\\ssafy_spec_pjt\\\\subtitle\\\\data\\\\refined_text\\\\\"\n",
    "\n",
    "# 자막 파일 폴더\n",
    "file_list = os.listdir(file_path)\n",
    "\n",
    "# 자막 파일 목록\n",
    "txt_files = [txt_file for txt_file in file_list]\n",
    "\n",
    "for txt_file in txt_files:\n",
    "  read_txt_file(txt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ 불용어 제거\n",
    "* 불용어 리스트 직접 설정  \n",
    "* nltk에 파일 생성 후 라이브러리 사용 (✔️)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 불용어 리스트 생성\n",
    "stop_words = []\n",
    "with open(\"stop_words_japanese.txt\", \"r\", encoding=\"UTF8\") as f :\n",
    "  lines = f.readlines()\n",
    "  # for stop_word in lines:\n",
    "  #   stop_words.append(stop_word.strip())\n",
    "  stop_words = [stop_word.strip() for stop_word in lines]\n",
    "\n",
    "# 생성된 불용어 리스트 확인\n",
    "print(\"불용어: \", stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ 사전 정보 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: C:\\Users\\sodud\\AppData\\Roaming\\Python\\Python39\\site-packages\\unidic_lite\\dicdir\\sys.dic\n",
      "charset: utf8\n",
      "size: 756264\n",
      "type: 0\n",
      "lsize: 5981\n",
      "rsize: 5981\n",
      "version: 102\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "try:\n",
    "  # 품사 분석\n",
    "  t = MeCab.Tagger()\n",
    "  \n",
    "  # 일본어 사전 정보 출력\n",
    "  d = t.dictionary_info()\n",
    "  while d:\n",
    "    print(\"filename: %s\" % d.filename)\n",
    "    print(\"charset: %s\" %  d.charset)\n",
    "    print(\"size: %d\" %  d.size)\n",
    "    print(\"type: %d\" %  d.type)\n",
    "    print(\"lsize: %d\" %  d.lsize)\n",
    "    print(\"rsize: %d\" %  d.rsize)\n",
    "    print(\"version: %d\" %  d.version)\n",
    "    d = d.next\n",
    "\n",
    "\n",
    "except RuntimeError as e:\n",
    "  print(\"RuntimeError:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import MeCab\n",
    "import neologdn\n",
    "from nltk.probability import FreqDist\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"font.family\"] = 'sans-serif'\n",
    "  \n",
    "# 포함 품사: 명사, 동사, 형용사, 부사\n",
    "CONTENT_WORD_POS = (\"名詞\", \"動詞\", \"形容詞\", \"副詞\")\n",
    "# 제외 품사: 접미사, 비자립, 대명사, 미지어\n",
    "IGNORE = (\"接尾\", \"非自立\", \"代名詞\", \"未知語\")\n",
    "\n",
    "def is_content_word(feature):\n",
    "  return feature.startswith(CONTENT_WORD_POS) and all(f not in IGNORE for f in feature.split(\",\")[:7])\n",
    "\n",
    "sentence = \"\"\n",
    "file_path = \"C:\\\\Users\\\\sodud\\\\study\\\\ssafy_spec_pjt\\\\subtitle\\\\data\\\\refined_text\\\\\"\n",
    "text = open(file_path + \"Getter_Robot_Go_001.txt\", \"r\", encoding=\"UTF8\")\n",
    "for line in text:\n",
    "  sentence += line\n",
    "\n",
    "try:\n",
    "  # 품사 분석\n",
    "  tagger = MeCab.Tagger()\n",
    "  \n",
    "  # 기본 출력 결과\n",
    "  # print(tagger.parse(sentence))\n",
    "  \n",
    "  sentence = neologdn.normalize(sentence,repeat = 2)\n",
    "  sentence = \"\".join([i for i in sentence if i.isalpha() or i.isspace()])\n",
    "  result = tagger.parseToNode(sentence)\n",
    "  \n",
    "  stop_words = []\n",
    "  with open(\"stop_words_japanese.txt\", \"r\", encoding=\"UTF8\") as f :\n",
    "    lines = f.readlines()\n",
    "    stop_words = [stop_word.strip() for stop_word in lines]\n",
    "    \n",
    "  content_words = []\n",
    "  \n",
    "  while result:\n",
    "    if is_content_word(result.feature):\n",
    "      lemma = result.feature.split(\",\")[7] if len(result.feature.split(\",\")) > 7 and result.feature.split(\",\")[7] != \"*\" else result.surface\n",
    "      if lemma not in stop_words:\n",
    "        content_words.append(lemma.split(\"-\")[0])\n",
    "    result = result.next\n",
    "  \n",
    "  print(content_words)\n",
    "  \n",
    "  fdist = FreqDist(content_words)\n",
    "  fdist.plot(10, cumulative=False)\n",
    "  \n",
    "except RuntimeError as e:\n",
    "    print(\"RuntimeError:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eebaf1173d8d9c3c4ee9a7b8bb1432a7f576348d6cb7a26bc263375fbc310797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
